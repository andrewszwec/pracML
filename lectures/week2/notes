===================================================
-- 	Week 2 Video Lecture Notes
===================================================

----------------------------------------------------
-- Video 1 - (Caret Package)
----------------------------------------------------
Caret package

createDataPartition()
createResample()
createTimeSlices

train
predict

confusionMatrix


----------------------------------------------------
-- Video 2 - (Data Slicing)
----------------------------------------------------
set.seed(1234)
createFolds( y, k=10, list = true, returnTrain=TRUE ) = k folds package

sapply(folds, length) = applies the length() function to each of the Folds vectors

returnTrain = TRUE means it returns the training set and the test set. returnTrain = FALSE mean it only returns the test set.

createResample() for bootstrapping

createTimeSlices(y, initialWindow=20, horizon=10)
- horizon are the 10 periods you want to predict



----------------------------------------------------
-- Video 3 - (Training Options)
----------------------------------------------------
preProcess
trControl {} boot632

google kappa



----------------------------------------------------
-- Video 4 - (Plotting Predictors)
----------------------------------------------------
featurePlot(x=training[,c("age","education",....], y=training$class, plot="pairs")

qplot(age, wage, colour=jobclass, data=training)

linear smooters in qplot + geom_smooth()


Break a variable into a group of N factors:
require(Hmisc)
cutWage <- cut2(training$wage, g=3)
table(cutWage)

2 plots side by side with 
grid.arrange()


cut2() is used to bin data into bins. It prints out the bins as a string

prop.table(t1, 1) - prints a table with the proportion of the variable across the rows....

density plots to plot continuous variables. Also allows you to overlay many distrobutions

When you plot look for imbalances in the data

outliers or weird groups

skewed variables that you need to transform with log etc 



=============================================================
-- Videos Week 2
=============================================================

-------------------------------------------------------------
-- Video 5: (Basic Preprocessing)
-------------------------------------------------------------

FIXING YOUR DATA BEFORE MODELLING!
- - - - - - - - - - - - - - - - - - 

Fixing skewed data
---------------------
centering and scaling fixes most problems in your data

"BoxCox" transformations take data and make it normally distributed

use qq plot to check how normal your data is qqnorm()!


Missing values in DATA
---------------------------
Use method "knnImpute" to fix missing values using K nearest neigbours algorithm


preprocess(x, method="knnImpute")

look at preprocessing with CARET!

look at links from week 2 slides!



-------------------------------------------------------------
-- Video 6: (Covariate Creation)
-------------------------------------------------------------

covariates = features = predictors

Create Dummy Variables
------------------------
training$jobclass
dummies <- dummyVars(wage~jobclass, data=training)
head(predict(dummies,newdata=training)) 

Getting rid of pointless variables:
nsv <- nearZeroVar(training, saveMetrics=TRUE)
nsv

ALLOW FOR CURVY MODEL FITTING:
--------------------------------
library(splines)
bsBasis <- bs(training$age, df=3)
bsBasis

Google feature extraction for data types eg emails, photos etc..

gam() package in caret for splines


------------------------------------------------------------
-- Video 7: (Preprocessing with PCA)
------------------------------------------------------------


M <- abs(cor(training))
diag(M) <- 0; % set diagonal elements to zeros since all variables have a correlation of 1 with themselves.

which(M > 0.8, arr.ind = T)

OUTPUT
		row	col
num415	34	32
num857	32	34


plot(training$num415, training$num857)

X = 0.71 x num415 + 0.71 x num857
Y = 0.71 x num415 - 0.71 x num857

plot(X, Y)

X = UDV' = Singular value decomposition

PCA
-----
prComp <- prcomp(training)
plot(prComp)

prComp$rotation (matrix of weightrf vairable contribution)


sometimes you need to do prComp(log(training )) to make you data gaussian

typeColour <- ((training$type == "spam")*1+1)
plot(X, Y, col=typeColour)


Steps of Cleaning Data:
- - - - - - - - - - - - - -
1. do log or boxcox transforms
2. knnImpute
3. Dummy Variables
4. nearZeroVar()
5. PCA

Always plot the data at every step to see if its working!

PLOTTING
--------
qqnorm()
bs()
gam()





------------------------------------------------------------
-- Video 8: (Predicting With Regression)
------------------------------------------------------------

you can plot the fit of the regression model using

lines(training$class, lm1$fitted, lwd=3)


# RMSE on training error
sqrt(sum(lm1$fitted - training$eruptions)^2)

# Test Error RMSE
sqrt(sum( predict(lm1, newdata = testdata) - training$eruptions)^2 )


PLOTTING PREDICTION INTERVALS 
see screen shot 6pm 15/4/2015

------------------------------------------------------------
-- Video 9: (Predicting With Regression and Multiple Covariates)
------------------------------------------------------------
This shows the data exploration process

qplot(age, wage, colour=jobcalss, data=training)

Do some plots with residuals and investigte outliers
- - - - - - - - - - - - - - - - - - - - - - - - - - - -
qplot(finMod$fitted, finMod$residuals, colour=race, data=training)


plot(finMod$residuals, pch=19)

residuals is the difference between the regression model and the real value. If there is a skew, you are missing a variable in your model

plot(wage, pred) # test your model!!


























































